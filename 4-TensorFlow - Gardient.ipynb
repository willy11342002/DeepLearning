{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們前面兩章都使用的簡單地梯度下降法, 這章我們試著更深入了解梯度下降, 試著解析這種做法的利弊.  \n",
    "在一開始我們先試著透過新的運算來繪製誤差曲面, 這要仰賴前面的模型, 而這會幫助我們更了解整個梯度的運作.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目標\n",
    "* 學習讀取`pretrained model`(如果上一章練習不夠)\n",
    "* 學習對`pretrained model`進行修改\n",
    "* 更深刻了解`梯度 (gardient)`\n",
    "* `優化器 optimization`的簡單介紹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引\n",
    "### [1 讀取模型](#1.-讀取模型)\n",
    "### [2 修改模型](#2.-修改模型)\n",
    "### [3 繪製誤差曲面](#3.-繪製誤差曲面)\n",
    "### [4 優化器](#4.-優化器)\n",
    "[4.1 Momentum](#4.1-Momentum)  \n",
    "[4.2 AdaGrad](#4.2-AdaGrad)  \n",
    "[4.3 AdaDelta](#4.3-AdaDelta)  \n",
    "[4.4 RMSProp](#4.4-RMSProp)  \n",
    "[4.5 Adam](#4.5-Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 讀取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為了簡化程式碼, 我把inference, loss寫到my_model這個python程式中, 就不用在這裡定義.\n",
    "from my_model import inference, loss, training, evaluate\n",
    "\n",
    "import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 載入資料, 記得one_hot, 我常常忘記\n",
    "mnist = input_data.read_data_sets('datasets/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入模型, 動作都跟上一章提到的一樣.  \n",
    "記得x, y都要拿出來, 因為我們要用feed_dict來替換.  \n",
    "千萬要記得, 不要使用新的變數去定義, 看圖會發現我們需要本來的x, y才能正確`feed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/ml_logs\\model-checkpoint-64350\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "saver = tf.train.import_meta_graph('logs/ml_logs/model-checkpoint-64350.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('logs/ml_logs'))\n",
    "\n",
    "g = tf.get_default_graph()\n",
    "\n",
    "x = g.get_tensor_by_name('x:0')\n",
    "y = g.get_tensor_by_name('y:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取出隱藏層的神經元跟權重向量, 這時候取出的是我們訓練完畢的權重.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list_opt = ['hidden_1/w:0', 'hidden_1/b:0', 'hidden_2/w:0', 'hidden_2/b:0', 'output/w:0', 'output/b:0']\n",
    "var_list_opt = [v for v in tf.trainable_variables() if v.name in var_list_opt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 修改模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們或許會因為某些需求, 先將某些步驟訓練完成後加進其他模型中,  \n",
    "或者我們需要使用別人訓練好的模型, 又或者我們只是想要對模型做一些微調.  \n",
    "這時候我們就需要試著去修改別人寫好的模型.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 隨機初始化一個同樣結構的模型, 等等就可以透過這個未訓練的模型來看到整個梯度的曲線.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用variable_scope()方法, 可以對變數名稱動手腳, 不會真正使用同名的變數\n",
    "with tf.variable_scope('ml_rand') as scope:\n",
    "    h1_rand, h2_rand, output_rand = inference(x)\n",
    "    cost_rand = loss(output_rand, y)\n",
    "    \n",
    "    # 這邊由於會重複名字, 所以要告訴它我要用重複名字的變數, 不燃tensorflow會抱怨\n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    var_list_rand = [\"hidden_1/w\", \"hidden_1/b\", \"hidden_2/w\", \"hidden_2/b\", \"output/w\", \"output/b\"]\n",
    "    var_list_rand = [tf.get_variable(v) for v in var_list_rand]\n",
    "    \n",
    "    # 接著我們要針對這些變數初始化\n",
    "    init_op = tf.variables_initializer(var_list_rand)\n",
    "    sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義一個全新的運算內容, 我們設定了佔位符`alpha`.  \n",
    "透過`alpha`的改變, 我們等等可以看到一個二維的梯度曲面.  \n",
    "有興趣的話可以試著做出三維, 不過權重的初始化或許就不能使用隨機了.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同樣給了新的運算單元一個獨特的命名空間, 這會讓裡面定義的運算都加上這個名稱前綴.\n",
    "with tf.variable_scope('ml_inter') as scope:\n",
    "    # 加入alpha的佔位符, shape=[1, 1], 這代表不論傳入多少x, y, 我們都用同一個alpha去計算\n",
    "    alpha = tf.placeholder('float', [1, 1])\n",
    "    \n",
    "    # 設計運算元的內容, 每個權重與神經元都被設計到一個方向上\n",
    "    # alpha趨近於0的時候, 計算出的結果會落在我們訓練完畢的模型上\n",
    "    # alpha遠離0的時候, 結果則會遠離我們本來訓練完成的結果\n",
    "    h1_W_inter = var_list_opt[0] * (1 - alpha) + var_list_rand[0] * (alpha)\n",
    "    h1_b_inter = var_list_opt[1] * (1 - alpha) + var_list_rand[1] * (alpha)\n",
    "    h2_W_inter = var_list_opt[2] * (1 - alpha) + var_list_rand[2] * (alpha)\n",
    "    h2_b_inter = var_list_opt[3] * (1 - alpha) + var_list_rand[3] * (alpha)\n",
    "    o_W_inter = var_list_opt[4] * (1 - alpha) + var_list_rand[4] * (alpha)\n",
    "    o_b_inter = var_list_opt[5] * (1 - alpha) + var_list_rand[5] * (alpha)\n",
    "    \n",
    "    # 將修改完得節點與權重丟到relu運算, 這步驟僅僅仿照原始的模型\n",
    "    h1_inter = tf.nn.relu(tf.matmul(x, h1_W_inter) + h1_b_inter)\n",
    "    h2_inter = tf.nn.relu(tf.matmul(h1_inter, h2_W_inter) + h2_b_inter)\n",
    "    o_inter = tf.nn.relu(tf.matmul(h2_inter, o_W_inter) + o_b_inter)\n",
    "    \n",
    "    # 使用新的輸出來計算誤差\n",
    "    cost_inter = loss(o_inter, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 繪製誤差曲面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了新的運算單元後我們就可以帶入不同的數字進去.  \n",
    "這個新的運算單元我們不訓練, 而是透過帶入不同的alpha值, 了解當下的誤差.  \n",
    "整個運算單元的概念再複習一下:  \n",
    "1. 首先我們有一個隨機初始化的模型, `model rand`\n",
    "2. 接著我們有一個學習完畢後的模型, `model opt`\n",
    "3. 透過修改`alpha`, 誤差會在這兩個模型連成的直線上移動\n",
    "4. 換句話說我們強迫誤差曲面從高維度, 降成了二維的空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一個 numpy array 來接收在不同 alpha 下的誤差\n",
    "results = np.zeros( np.arange(-2, 2, 0.02).shape )\n",
    "\n",
    "# 將 alpha 設定在 -2~2 之間, 每隔 0.02 就計算一次當下的誤差\n",
    "for idx, a in enumerate( np.arange(-2, 2, 0.02) ):\n",
    "    # 先將每次要傳進去的 x, y, alpha 寫在 feed_dict中\n",
    "    feed_dict = {\n",
    "        x: mnist.test.images,\n",
    "        y: mnist.test.labels,\n",
    "        alpha: [[a]],\n",
    "    }\n",
    "    \n",
    "    # 每次算出誤差, 就把剛剛先填上零的 numpy array 對應的值修改掉\n",
    "    cost = sess.run(cost_inter, feed_dict)\n",
    "    results[idx]=cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 畫出我們剛剛算出來的數字\n",
    "plt.plot(np.arange(-2, 2, 0.02), results, 'ro')\n",
    "plt.ylabel('Incurred Error')\n",
    "plt.xlabel('Alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上圖, 我們把曲面強制設定在了一個方向上.  \n",
    "這讓我們看到了`鞍點`, 同時這也解釋了上一章有時候會沒辦法收斂的原因, 其實就只是訓練不夠而已, 總是能夠繼續前進的.  \n",
    "也就是說, 如果我們能夠確定前進的方向, 那麼`鞍點`是不能阻止我們的.  \n",
    "此外, 在2014年還有一個研究指出, 在能夠確定方向的前提下, `局部極小值`並不會造成梯度下降過程的麻煩.  \n",
    "***我們最終只有一個問題, 那就是我們要找到前進的方向.***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 優化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定參數, 跑 50 次迭代, 每個小批次設定為128, 每 10 次迭代我們才印一次誤差\n",
    "epochs=50 \n",
    "batch_size=128\n",
    "state=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊我們重新定義訓練方法, 優化器我們用參數的方式傳遞.  \n",
    "這能夠方便我們傳入不同的優化器, 使用不同的方式做迭代."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step, optimizer):   \n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    \n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們將`創建模型`、`訓練模型`、`印出預測`的過程包成一個方法.  \n",
    "這樣等等我們就不用重複打上這麼長的程式碼, 可以很簡潔的表達我們每次跑出一個新的模型的過程.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_a_model(name, optimizer):\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "        global_step_momentum = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        h1_momentum, h2_momentum, output_momentum = inference(x)\n",
    "        cost_momentum = loss(output_momentum, y)\n",
    "        training_momentum = training(cost_momentum, global_step_momentum, optimizer)\n",
    "        evaluate_momentum = evaluate(output_momentum, y)\n",
    "\n",
    "        var_list_momentum = ['hidden_1/w', 'hidden_1/b', 'hidden_2/w', 'hidden_2/b', 'output/w', 'output/b']\n",
    "        var_list_momentum = [tf.get_variable(v) for v in var_list_momentum]\n",
    "\n",
    "        init_momentum = tf.global_variables_initializer()\n",
    "        sess.run(init_momentum)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_batch = mnist.train.num_examples//batch_size\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                mbatch_x, mbatch_y = mnist.train.next_batch(batch_size)\n",
    "                sess.run(training_momentum, feed_dict={x:mbatch_x, y:mbatch_y})\n",
    "\n",
    "            if epoch % state == 0:\n",
    "                train_accuracy = sess.run(evaluate_momentum, feed_dict={x:mnist.train.images, y:mnist.train.labels})\n",
    "                val_accuracy = sess.run(evaluate_momentum, feed_dict={x:mnist.validation.images, y:mnist.validation.labels})\n",
    "                print('Epochs:', '%04d' % (epoch+1), 'train_error:', (1-train_accuracy), 'val_error:', (1-val_accuracy))\n",
    "\n",
    "        print('Optimization Finished!')\n",
    "\n",
    "        accuracy = sess.run(evaluate_momentum, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "        print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "源自於當我們丟一顆球在一個漏斗中, 這顆球會隨著坡度向下最終會到達漏斗底部的這個概念.  \n",
    "在迭代的過程中加入了動能, 每次更新方向時判斷現在要繼續加速還是現在要減速.  \n",
    "如果連續的往同樣方向前進, 那速率會提高, 我們就能更快收斂.  \n",
    "但是如果方向改變, 那速率會減緩.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0001 train_error: 0.0641090869904 val_error: 0.0612000226974\n",
      "Epochs: 0011 train_error: 0.00530910491943 val_error: 0.01859998703\n",
      "Epochs: 0021 train_error: 9.08970832825e-05 val_error: 0.0162000060081\n",
      "Epochs: 0031 train_error: 7.2717666626e-05 val_error: 0.0159999728203\n",
      "Epochs: 0041 train_error: 7.2717666626e-05 val_error: 0.0157999992371\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "# 使用 momentum 的演算法進行迭代\n",
    "optimizer = tf.train.MomentumOptimizer(0.02, 0.9)\n",
    "run_a_model('momentum', optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是在2011年被提出的演算法, 我們在前面的優化器中, 對各個權重都是用固定的`learning rate`.  \n",
    "但是我們也可以想像, 不同的參數重要性肯定是不一樣的, 因此考量每個參數用不同的`learning rate`總是一個很吸引人的想法.  \n",
    "`AdaGrad`就從這裡出發, 它對過去的梯度做了平方累加, 再開根號, 接著用一開始設定的`learning rate`除這個數.  \n",
    "得到的效果除了每個參數不同之外, 還會隨著迭代次數增加速率遞減, 所以一開始可以設定一個較大的`learning rate`.  \n",
    "但是也由於這個特性的關係, `AdaGrad`會讓速率下降, 在很多模型中表現會比較不好."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0001 train_error: 0.0543636083603 val_error: 0.0515999794006\n",
      "Epochs: 0011 train_error: 0.00543636083603 val_error: 0.0210000276566\n",
      "Epochs: 0021 train_error: 0.000309109687805 val_error: 0.0174000263214\n",
      "Epochs: 0031 train_error: 9.08970832825e-05 val_error: 0.0171999931335\n",
      "Epochs: 0041 train_error: 7.2717666626e-05 val_error: 0.0170000195503\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "# 使用 AdaGrad 進行迭代\n",
    "optimizer = tf.train.AdagradOptimizer(0.05)\n",
    "run_a_model('AdaGrad', optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 AdaDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是`AdaGrad`的修正版, 減緩了對於一開始設定的`learning rate`的依賴.  \n",
    "除此之外, 也減緩了速率持續下降導致更新速度變慢的問題.  \n",
    "最終的效果有點類似`momentum`, 會加速收斂的效果."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0001 train_error: 0.384345471859 val_error: 0.379599988461\n",
      "Epochs: 0011 train_error: 0.230309069157 val_error: 0.223399996758\n",
      "Epochs: 0021 train_error: 0.218854546547 val_error: 0.214200019836\n",
      "Epochs: 0031 train_error: 0.212890923023 val_error: 0.212599992752\n",
      "Epochs: 0041 train_error: 0.2088727355 val_error: 0.210799992085\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.7905\n"
     ]
    }
   ],
   "source": [
    "# 使用 AdaDelta 進行迭代\n",
    "optimizer = tf.train.AdadeltaOptimizer(0.1)\n",
    "run_a_model('AdaDelta', optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個演算法是前面的演算法的延伸, 加入了`衰減係數`決定要保留多久以前的值來計算.  \n",
    "整個演算法的效果大概介於`momentum`跟`adagrad`之間. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0001 train_error: 0.0480545163155 val_error: 0.0487999916077\n",
      "Epochs: 0011 train_error: 0.0253636240959 val_error: 0.0357999801636\n",
      "Epochs: 0021 train_error: 0.0135090947151 val_error: 0.0271999835968\n",
      "Epochs: 0031 train_error: 0.0131454467773 val_error: 0.0307999849319\n",
      "Epochs: 0041 train_error: 0.037290930748 val_error: 0.049399971962\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.8986\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(0.01)\n",
    "run_a_model('RMSProp', optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這又是組合了前面, 將`momentum`與`RMSProp`做了結合.  \n",
    "在整個更新過程中都會保持一個平穩的下降, 在近年越來越受歡迎.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0001 train_error: 0.0346545577049 val_error: 0.0364000201225\n",
      "Epochs: 0011 train_error: 0.00316363573074 val_error: 0.0199999809265\n",
      "Epochs: 0021 train_error: 0.00147271156311 val_error: 0.0175999999046\n",
      "Epochs: 0031 train_error: 0.00321817398071 val_error: 0.0207999944687\n",
      "Epochs: 0041 train_error: 0.0013090968132 val_error: 0.0189999938011\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9822\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "run_a_model('Adam', optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當我們的迭代碰上平緩曲面, 這種時候使用動能類型的優化器能夠加快我們脫離泥沼的速度.  \n",
    "而當碰上了陡峭的曲面, 使用平穩的優化器又能夠確保不要跑太遠.  \n",
    "就是因為不同的情況適合的不同的演算法, 所以有了各種各樣的優化器供我們選擇.  \n",
    "因應不同的情況知道怎麼選擇演算法固然是好事.  \n",
    "但是實際上在深度學習領域獲得重大突破的往往不是與誤差曲面進行搏鬥的這部分.  \n",
    "而是模型的創建, 因此這一篇放在入門是希望大家看完有點印象, 但是千萬不要鑽研在這上面."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
