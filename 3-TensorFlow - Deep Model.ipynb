{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 建構簡單深度學習模型\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目標\n",
    "* 使用tensorflow建構自己的`層 (layer)`\n",
    "* 使用`層 (layer)`建構深度網路  \n",
    "* 使用套件幫助自己進行模型檢測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引\n",
    "## [1 讀取資料](#1.-讀取資料)\n",
    "## [2 建立模型](#2.-建立模型)\n",
    "[2.1 層 Layer](#2.1-層-Layer)  \n",
    "[2.2 推測](#2.2-推測)  \n",
    "[2.3 誤差](#2.3-誤差)  \n",
    "[2.4 訓練](#2.4-訓練)  \n",
    "[2.5 評估](#2.5-評估)  \n",
    "[2.6 模型訓練](#2.6-模型訓練)  \n",
    "## [3 模型存取](#3.-模型存取)\n",
    "[3.1 Tensorflow模型文件](#3.1-Tensorflow模型文件)\n",
    "* [3.1.1 meta檔](#3.1.1-meta-檔)  \n",
    "* [3.1.2 index, data 檔](#3.1.2-index,-data-檔)  \n",
    "* [3.1.3 checkpoint 檔](#3.1.3-checkpoint-檔)  \n",
    "\n",
    "[3.2 保存模型](#3.2-保存模型)   \n",
    "[3.3 讀取模型](#3.3-讀取模型)\n",
    "* [3.3.1 讀圖](#3.3.1-讀圖)\n",
    "* [3.3.2 載參數](#3.3.2-載參數)\n",
    "\n",
    "## [4 模型檢測](#4.-模型存取)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 讀取資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一章我們透過一大堆的操作，讀取並轉換我們的手寫數字資料集，讓我們得以丟進模型中訓練。  \n",
    "但是事實上，手寫數字作為一個新手入門最強大的數字集，google那幫人已經寫好了程式碼方便大家讀取。  \n",
    "在開始之前，還是要介紹一下手寫數字集的官網[Yann LeCun's website](http://yann.lecun.com/exdb/mnist/)。  整份資料被分為60000筆訓練資料, 10000筆測試資料, 由於需要驗證因此訓練集又被切出5000筆驗證資料。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets\\train-images-idx3-ubyte.gz\n",
      "Extracting datasets\\train-labels-idx1-ubyte.gz\n",
      "Extracting datasets\\t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import input_data\n",
    "\n",
    "# 使用 input_data 的 read_data_sets() 方法, 指定資料集位置, 如果沒有會直接下載\n",
    "mnist = input_data.read_data_sets('datasets/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果大家有養成好習慣的話, 應該還是要如同上一章一樣做資料的檢查。  \n",
    "事實上我自己在使用時也是都有做的, 只是最後整理做成筆記本的時候拿掉了。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 層 Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度網路, 顧名思義就是深度比別人深的神經網路.  \n",
    "如果在很複雜的模型中, 我們會把很多的`層 layer`放在一個`區塊 block`中.  \n",
    "但是這邊還不需要使用這種網路, 我們先學會如何設計一個`層 layer`.  \n",
    "首先要先了解有關於層的概念:  \n",
    "1. `層`是一個或多個神經元的組合\n",
    "2. `層`必須要搭配非線性的激活函數, 如`sigmoid`, `tenh`, `relu`等.\n",
    "3. `層`與`層`之間的連結是可以自己決定的, 可以跨`層`傳遞, 也可以不傳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(inputs, weight_shape, bias_shape):\n",
    "    '''定義一個 relu 層'''\n",
    "    \n",
    "    # 設定初始化方法, 由於 relu 的特性, 所以使用常態分配來初始化\n",
    "    w_init = tf.random_normal_initializer(stddev=(2.0/weight_shape[0])**0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    \n",
    "    # 定義每一層有 bias 個神經元\n",
    "    w = tf.get_variable('w', weight_shape, initializer=w_init)\n",
    "    b = tf.get_variable('b', bias_shape, initializer=bias_init)\n",
    "    \n",
    "    # 回傳使用 relu 運算\n",
    "    return tf.nn.relu( tf.matmul(inputs, w) + b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 推測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    '''定義推測的步驟, 傳入一筆或多筆資料, 經過兩個隱藏層後, 直接輸出推測結果'''\n",
    "    \n",
    "    # 定義兩個隱藏層, 兩個隱藏層都有256個神經元, 與上一層全連結\n",
    "    # 特別使用 variable_scope 這個特殊的方法, 這在圖上可以清楚看到\n",
    "    # 後面會看到, 當我們要取得裡面的節點, 就需要加上這個變數區域\n",
    "    with tf.variable_scope('hidden_1'):\n",
    "        hidden_1 = layer(x, [784, 256], [256])\n",
    "    with tf.variable_scope('hidden_2'):\n",
    "        hidden_2 = layer(hidden_1, [256, 256], [256])\n",
    "        \n",
    "    # 定義一個輸出層, 與上一層全連結, 特別注意沒有softmax運算\n",
    "    with tf.variable_scope('output'):\n",
    "        output = layer(hidden_2, [256, 10], [10])\n",
    "        \n",
    "    return hidden_1, hidden_2, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    '''定義誤差函數計算的步驟, 這邊使用的是交叉嫡'''\n",
    "    \n",
    "    # 這個模型在這時候才使用 softmax 並進行交叉嫡運算\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y)    \n",
    "    \n",
    "    # 直接將所有的數字揉在一起做平均\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step, lr):   \n",
    "    '''定義訓練的步驟, 用梯度下降法'''\n",
    "    \n",
    "    # 紀錄過程\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # 定義訓練的方法, 使用梯度下降法\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    \n",
    "    # 進行誤差最小化任務\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    \n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    '''定義評估的方式, 輸入標籤以及預測標籤, 輸出準確率'''\n",
    "    \n",
    "    # 找出標籤與預測標籤的最大信心水準, 比較是否相同\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # 沿著 0 維度降維, 算出一個準確率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 紀錄過程\n",
    "    tf.summary.scalar('validation_error', (1. - accuracy))\n",
    "    \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義模型所需參數\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Train Error: 0.333127260208 Validation Error: 0.330999970436\n",
      "Epoch: 0002 Train Error: 0.215600013733 Validation Error: 0.212999999523\n",
      "Epoch: 0003 Train Error: 0.199763655663 Validation Error: 0.194599986076\n",
      "Epoch: 0004 Train Error: 0.192818164825 Validation Error: 0.1833999753\n",
      "Epoch: 0005 Train Error: 0.184745430946 Validation Error: 0.17720001936\n",
      "Epoch: 0006 Train Error: 0.180036365986 Validation Error: 0.175199985504\n",
      "Epoch: 0007 Train Error: 0.175709068775 Validation Error: 0.171000003815\n",
      "Epoch: 0008 Train Error: 0.171345472336 Validation Error: 0.167800009251\n",
      "Epoch: 0009 Train Error: 0.0670182108879 Validation Error: 0.0649999976158\n",
      "Epoch: 0010 Train Error: 0.0617454648018 Validation Error: 0.0565999746323\n",
      "Epoch: 0011 Train Error: 0.0585272908211 Validation Error: 0.0541999936104\n",
      "Epoch: 0012 Train Error: 0.0563818216324 Validation Error: 0.0522000193596\n",
      "Epoch: 0013 Train Error: 0.0539636611938 Validation Error: 0.0508000254631\n",
      "Epoch: 0014 Train Error: 0.0515454411507 Validation Error: 0.0483999848366\n",
      "Epoch: 0015 Train Error: 0.0496545433998 Validation Error: 0.0469999909401\n",
      "Epoch: 0016 Train Error: 0.0472363829613 Validation Error: 0.0451999902725\n",
      "Epoch: 0017 Train Error: 0.0457090735435 Validation Error: 0.0454000234604\n",
      "Epoch: 0018 Train Error: 0.0445091128349 Validation Error: 0.0433999896049\n",
      "Epoch: 0019 Train Error: 0.0430727005005 Validation Error: 0.0433999896049\n",
      "Epoch: 0020 Train Error: 0.0411999821663 Validation Error: 0.0415999889374\n",
      "Epoch: 0021 Train Error: 0.0397818088531 Validation Error: 0.0397999882698\n",
      "Epoch: 0022 Train Error: 0.0386182069778 Validation Error: 0.0382000207901\n",
      "Epoch: 0023 Train Error: 0.0372181534767 Validation Error: 0.0379999876022\n",
      "Epoch: 0024 Train Error: 0.0362545251846 Validation Error: 0.0361999869347\n",
      "Epoch: 0025 Train Error: 0.0347636342049 Validation Error: 0.0356000065804\n",
      "Epoch: 0026 Train Error: 0.0341636538506 Validation Error: 0.0342000126839\n",
      "Epoch: 0027 Train Error: 0.0338545441628 Validation Error: 0.0342000126839\n",
      "Epoch: 0028 Train Error: 0.0319636464119 Validation Error: 0.0343999862671\n",
      "Epoch: 0029 Train Error: 0.0320181846619 Validation Error: 0.035000026226\n",
      "Epoch: 0030 Train Error: 0.0313636660576 Validation Error: 0.0342000126839\n",
      "Epoch: 0031 Train Error: 0.0298181772232 Validation Error: 0.0329999923706\n",
      "Epoch: 0032 Train Error: 0.0286727547646 Validation Error: 0.0332000255585\n",
      "Epoch: 0033 Train Error: 0.0278182029724 Validation Error: 0.0325999855995\n",
      "Epoch: 0034 Train Error: 0.0276545286179 Validation Error: 0.0321999788284\n",
      "Epoch: 0035 Train Error: 0.0271818041801 Validation Error: 0.0310000181198\n",
      "Epoch: 0036 Train Error: 0.0270909070969 Validation Error: 0.0296000242233\n",
      "Epoch: 0037 Train Error: 0.0254363417625 Validation Error: 0.0307999849319\n",
      "Epoch: 0038 Train Error: 0.0251091122627 Validation Error: 0.0299999713898\n",
      "Epoch: 0039 Train Error: 0.0243636369705 Validation Error: 0.0297999978065\n",
      "Epoch: 0040 Train Error: 0.0238000154495 Validation Error: 0.0296000242233\n",
      "Epoch: 0041 Train Error: 0.0227817893028 Validation Error: 0.0292000174522\n",
      "Epoch: 0042 Train Error: 0.0226727128029 Validation Error: 0.0281999707222\n",
      "Epoch: 0043 Train Error: 0.0226545333862 Validation Error: 0.0292000174522\n",
      "Epoch: 0044 Train Error: 0.0217090845108 Validation Error: 0.0278000235558\n",
      "Epoch: 0045 Train Error: 0.0216181874275 Validation Error: 0.0278000235558\n",
      "Epoch: 0046 Train Error: 0.020890891552 Validation Error: 0.027999997139\n",
      "Epoch: 0047 Train Error: 0.0208545327187 Validation Error: 0.0274000167847\n",
      "Epoch: 0048 Train Error: 0.0201272964478 Validation Error: 0.0274000167847\n",
      "Epoch: 0049 Train Error: 0.0195636153221 Validation Error: 0.0278000235558\n",
      "Epoch: 0050 Train Error: 0.0190727114677 Validation Error: 0.0256000161171\n",
      "Epoch: 0051 Train Error: 0.0188181996346 Validation Error: 0.0270000100136\n",
      "Epoch: 0052 Train Error: 0.0186727046967 Validation Error: 0.0261999964714\n",
      "Epoch: 0053 Train Error: 0.0178545713425 Validation Error: 0.0264000296593\n",
      "Epoch: 0054 Train Error: 0.0173818469048 Validation Error: 0.0266000032425\n",
      "Epoch: 0055 Train Error: 0.0173272490501 Validation Error: 0.0257999897003\n",
      "Epoch: 0056 Train Error: 0.0167090892792 Validation Error: 0.0248000025749\n",
      "Epoch: 0057 Train Error: 0.0164545178413 Validation Error: 0.0260000228882\n",
      "Epoch: 0058 Train Error: 0.0159999728203 Validation Error: 0.0260000228882\n",
      "Epoch: 0059 Train Error: 0.0158181786537 Validation Error: 0.0267999768257\n",
      "Epoch: 0060 Train Error: 0.0153636336327 Validation Error: 0.025200009346\n",
      "Epoch: 0061 Train Error: 0.0151636600494 Validation Error: 0.0243999958038\n",
      "Epoch: 0062 Train Error: 0.0145636200905 Validation Error: 0.0234000086784\n",
      "Epoch: 0063 Train Error: 0.0145090818405 Validation Error: 0.0246000289917\n",
      "Epoch: 0064 Train Error: 0.0144545435905 Validation Error: 0.025200009346\n",
      "Epoch: 0065 Train Error: 0.0138909220695 Validation Error: 0.0238000154495\n",
      "Epoch: 0066 Train Error: 0.0133090615273 Validation Error: 0.0239999890327\n",
      "Epoch: 0067 Train Error: 0.0134363770485 Validation Error: 0.0238000154495\n",
      "Epoch: 0068 Train Error: 0.0129454731941 Validation Error: 0.0234000086784\n",
      "Epoch: 0069 Train Error: 0.0125272870064 Validation Error: 0.0234000086784\n",
      "Epoch: 0070 Train Error: 0.0123090744019 Validation Error: 0.0242000222206\n",
      "Epoch: 0071 Train Error: 0.0119272470474 Validation Error: 0.0239999890327\n",
      "Epoch: 0072 Train Error: 0.012036383152 Validation Error: 0.0228000283241\n",
      "Epoch: 0073 Train Error: 0.0117636322975 Validation Error: 0.0228000283241\n",
      "Epoch: 0074 Train Error: 0.0113636255264 Validation Error: 0.0235999822617\n",
      "Epoch: 0075 Train Error: 0.0111091136932 Validation Error: 0.0230000019073\n",
      "Epoch: 0076 Train Error: 0.0108909010887 Validation Error: 0.0228000283241\n",
      "Epoch: 0077 Train Error: 0.0110727548599 Validation Error: 0.0234000086784\n",
      "Epoch: 0078 Train Error: 0.0105817914009 Validation Error: 0.0230000019073\n",
      "Epoch: 0079 Train Error: 0.0102363824844 Validation Error: 0.022000014782\n",
      "Epoch: 0080 Train Error: 0.0102182030678 Validation Error: 0.0234000086784\n",
      "Epoch: 0081 Train Error: 0.0099818110466 Validation Error: 0.0228000283241\n",
      "Epoch: 0082 Train Error: 0.0095454454422 Validation Error: 0.022400021553\n",
      "Epoch: 0083 Train Error: 0.00950908660889 Validation Error: 0.0234000086784\n",
      "Epoch: 0084 Train Error: 0.00959998369217 Validation Error: 0.0234000086784\n",
      "Epoch: 0085 Train Error: 0.00958180427551 Validation Error: 0.0225999951363\n",
      "Epoch: 0086 Train Error: 0.00918179750443 Validation Error: 0.0235999822617\n",
      "Epoch: 0087 Train Error: 0.00852727890015 Validation Error: 0.0234000086784\n",
      "Epoch: 0088 Train Error: 0.0089818239212 Validation Error: 0.0225999951363\n",
      "Epoch: 0089 Train Error: 0.00845456123352 Validation Error: 0.0217999815941\n",
      "Epoch: 0090 Train Error: 0.00805455446243 Validation Error: 0.0230000019073\n",
      "Epoch: 0091 Train Error: 0.00809091329575 Validation Error: 0.022400021553\n",
      "Epoch: 0092 Train Error: 0.00781816244125 Validation Error: 0.0228000283241\n",
      "Epoch: 0093 Train Error: 0.007672727108 Validation Error: 0.022400021553\n",
      "Epoch: 0094 Train Error: 0.00763636827469 Validation Error: 0.0230000019073\n",
      "Epoch: 0095 Train Error: 0.00729089975357 Validation Error: 0.022000014782\n",
      "Epoch: 0096 Train Error: 0.00770908594131 Validation Error: 0.0230000019073\n",
      "Epoch: 0097 Train Error: 0.00714546442032 Validation Error: 0.0228000283241\n",
      "Epoch: 0098 Train Error: 0.00730907917023 Validation Error: 0.0216000080109\n",
      "Epoch: 0099 Train Error: 0.00690907239914 Validation Error: 0.0231999754906\n",
      "Epoch: 0100 Train Error: 0.00683635473251 Validation Error: 0.021399974823\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9773\n"
     ]
    }
   ],
   "source": [
    "# 開啟新的圖(graph), 這樣會在結束運算後自動關閉\n",
    "# 僅因測試使用, 方便釋放資源以及模型自動清空, 重新執行才不會出現已經存在的錯誤\n",
    "# 如果需要保存變數資料, 請不要使用這個語法\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 設定 x, y 的佔位符, 這樣可以因應不同情況做替換\n",
    "    x = tf.placeholder('float', [None, 784], name='x')\n",
    "    y = tf.placeholder('float', [None, 10], name='y')\n",
    "\n",
    "    # 初始化從零開始, 計算目前訓練的次數\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # 呼叫上方定義的四個模型組成要素\n",
    "    hidden_1, hidden_2, output = inference(x)\n",
    "    cost = loss(output, y)\n",
    "    train_op = training(cost, global_step, lr)\n",
    "    eval_op = evaluate(output, y)\n",
    "\n",
    "    # 設定儲存節點跟目標資料夾\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver(max_to_keep=1) # 只保留最後一筆記錄檔\n",
    "    summary_writer = tf.summary.FileWriter('logs/ml_logs', graph=g) \n",
    "\n",
    "    # 開啟會話, 進行變數的初始化.\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # 開始訓練, 這個迴圈會重複進行直到訓練結束\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # 由於是小批次訓練, 這邊設定了每個epoch會更新的次數\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            # 呼叫上方定義的小批次傳入方法, 取得當前的 (x, y)\n",
    "            mbatch_x, mbatch_y = mnist.train.next_batch(batch_size)\n",
    "            # 由會話開啟訓練, 由於我們使用了placeholder, 所以feed_dict是一定要給的\n",
    "            sess.run(train_op, feed_dict={x:mbatch_x, y:mbatch_y})\n",
    "\n",
    "        # 呼叫評估方法, 計算出train, val的準確度, 印出誤差\n",
    "        train_accuracy = sess.run(eval_op, feed_dict={x:mnist.train.images, y:mnist.train.labels})\n",
    "        val_accuracy = sess.run(eval_op, feed_dict={x:mnist.validation.images, y:mnist.validation.labels})\n",
    "        print('Epoch:', '%04d' % (epoch+1), 'Train Error:', (1-train_accuracy), 'Validation Error:', (1-val_accuracy))\n",
    "\n",
    "        # 執行我們設定的紀錄節點, 並把結果寫到指定的資料夾\n",
    "        summary_str = sess.run(summary_op, feed_dict={x:mbatch_x, y:mbatch_y})\n",
    "        summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "        saver.save(sess, 'logs/ml_logs/model-checkpoint', global_step=global_step)\n",
    "\n",
    "    # 結束\n",
    "    print('Optimization Finished!')\n",
    "\n",
    "    # 呼叫評估方法, 計算test資料準確度並印出\n",
    "    accuracy = sess.run(eval_op, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "    print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們現在訓練完成了, 狀態好的話應該是可以得到在測試資料中有將近98%的準確率的模型.  \n",
    "如果跑出來結果很差, 只有70%或80%的話, 多試幾次應該就可以.  \n",
    "會有這個問題一方面是因為我們使用了很簡單的梯度下降, 如果誤差曲面很平緩, 就會造成參數沒辦法收斂.  \n",
    "當然還有另外一個問題就是我們訓練次數設太少了, 我有點偷懶所以只訓練100次, 還把學習速率稍微調高了.  \n",
    "這兩個問題暫時先擺著, 這是為了能夠快速進行這個教程演示.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型存取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在tensorflow養成過程中, 我們有時候需要使用別人訓練好的模型, 稱作`pretrained model`.  \n",
    "或是我們模型訓練好, 還需要做其他的處理或是取得其中權重等動作.  \n",
    "因此我們需要了解tensorflow怎麼保存我們訓練好的模型數據."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tensorflow模型文件\n",
    "由於我們有在訓練中持續保存模型, 我們現在可以打開 checkpoint_dir 目錄.  \n",
    "可以看到以下幾個模型保存的內容（如果存檔的參數都跟我設定一樣的話）：\n",
    "* checkpoint\n",
    "* model-checkpoint.meta\n",
    "* model-checkpoint.index\n",
    "* model-checkpoint.data-00000-of-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 meta 檔\n",
    "meta文件是保存圖的結構, 包含變量, op, 集合等."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 index, data 檔\n",
    "這兩個檔案是模型檔, 二進制文件, 保存了權重、神經元、梯度等變量。  \n",
    "在tensorflow 0.11版之前, 保存在.ckpt文件中。  \n",
    "在新版中, 改為通過兩個文件保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 checkpoint 檔\n",
    "這是訓練紀錄檔, 或者我們可以認為這是版本紀錄檔, 它map不同時期的模型其對應的模型檔.    \n",
    "也就是我們在`inference`的時候, 可以透過修改這個檔案, 指定使用什麼時候的model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 保存模型\n",
    "tensorflow 提供了 `tf.train.Saver` 來保存模型.  \n",
    "要特別注意, tensorflow 將變量存放於 `Session` 中.  \n",
    "也就是說當我們使用 `with` 寫法, 一旦離開了 `Session` 環境, 變數就隨風東流了.  \n",
    "所以如果前面的語法中省略了保存動作, 現在想要拿到同樣的模型, 是不可能了QQ.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於剛剛我們確實有仔細的存出我們的模型, 因此現在就只需要展示程式碼就好囉.  \n",
    "`saver = tf.train.Saver()`  \n",
    "`saver.save(sess, 'ml_logs/model-checkpoint', global_step=global_step)`  \n",
    "這是我們剛剛主要使用的程式碼, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在實際使用時, 我們可能要持續保存模型, 但是由於圖是不變的, 可以通過下面方式設定不保存圖：  \n",
    "`saver.save(sess, 'ml_logs/model-checkpoint',\n",
    "                   global_step=global_step,\n",
    "                   write_meta_graph=False)`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "還有一種也很實用的方式, 如果我們希望每小時保存一次模型, 且只保留最近的5個模型：   \n",
    "`tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 讀取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面我們提到, tensorflow將模型數據分開保存為不同的檔案.  \n",
    "因此, 我們在讀取模型的時候也要分為兩步讀取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 讀圖\n",
    "既然模型已經存下了圖的結構, 那麼我們可以使用下方的程式碼直接將圖讀出就好.  \n",
    "比較怕存的時候沒有存到圖, 這一定要特別小心, 建議跑完訓練後都要檢查一下目錄.  \n",
    "\n",
    "`saver = tf.train.import_meta_graph('ml_logs/model-checkpoint-42900.meta')`\n",
    "\n",
    "執行上方程式碼後, 我們就順利把圖讀進來了.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 載參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "僅僅只有圖沒有用, 我們模型更重要的是其中保存的參數.  \n",
    "上方也特別提到, 變數值的存放需要仰賴`Session`.  \n",
    "因此我們要先創建好`Session`, 才能載入參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ml_logs\\model-checkpoint-42900\n"
     ]
    }
   ],
   "source": [
    "# 創建會話, 還是要注意這種寫法需要手動關閉才能釋放資源\n",
    "sess = tf.Session()\n",
    "\n",
    "# 讀取圖\n",
    "saver = tf.train.import_meta_graph('logs/ml_logs/model-checkpoint-42900.meta')\n",
    "\n",
    "# 讀取權重\n",
    "saver.restore(sess, tf.train.latest_checkpoint('logs/ml_logs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "執行上方的程式碼, 我們可以看到 tensorflow 告訴我們成功讀取了權重.  \n",
    "這時候我們就可以開始拿出模型的權重了.  \n",
    "開始之前通常都要完成下面的動作, 可以當作例行公事處理.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這個動作是將現在預設的圖放到python變數'g'(注意不是tensorflow變數)\n",
    "g = tf.get_default_graph()\n",
    "\n",
    "# 由於 placeholder 我們需要隨時給值, 所以把它們取出準備放不同的值\n",
    "# 注意名稱, 使用的是我們放在 tf.placeholder() 裡面的參數 name='?'\n",
    "# 這個名稱是唯一且不重複的, 為了讓我們命名不受限制, tensorflow 會自動加上索引\n",
    "x = g.get_tensor_by_name('x:0')\n",
    "y = g.get_tensor_by_name('y:0')\n",
    "\n",
    "# 設定 feed 字典\n",
    "feed_dict={x:mnist.test.images, y:mnist.test.labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成後, 我們就可以對模型做任何想做的事情了~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97729999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我們可以丟新的資料給模型預測\n",
    "\n",
    "# 拿到預測的節點\n",
    "# 注意名稱, 由於這個節點我們並沒有命名\n",
    "# tensorflow會自動命名, 要拿到名稱要看圖, 所以 tensorboard 還是很重要的.\n",
    "accuracy = g.get_tensor_by_name('Mean_1:0')\n",
    "# 運用會話, 執行節點, 塞入test資料, 跟剛剛的結果是一樣的~~\n",
    "sess.run(accuracy, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我們可以拿到任何隱藏層的輸出\n",
    "\n",
    "# 拿到第一個隱藏層的輸出\n",
    "# 注意名稱, 我們在 inference 的時候給了它變數區域, 因此要存取也要加上.\n",
    "h1 = g.get_tensor_by_name('hidden_1/Relu:0')\n",
    "\n",
    "# 運用會話, 執行節點, 塞入test資料\n",
    "# 可以看到輸出的 tensor.shape 是 [datas, 256] 的形狀, 不是一開始資料的 [datas, 784]\n",
    "sess.run(h1, feed_dict).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當然, 既然我們可以存取, 就意味著我們可以修改.  \n",
    "我們可以設計新的運算加入圖中, 這就是`pretrained model`的概念.  \n",
    "我們拿到已經訓練完畢的模型, 可能從某個節點改變其運作方向, 加入自己的設計.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型檢測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一章我們並沒有寫上模型的檢測.  \n",
    "最主要是一開始使用的是沒有隱藏層的簡單模型, 並不能深刻的了解深度學習到底在深什麼.  \n",
    "再加上上一章已經有了資料處理(雖然是很基礎的), 實在不想全都塞在同一個筆記本裡.  \n",
    "因此一直到這裡我們才進行真正的模型檢測.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = g.get_tensor_by_name('output/Relu:0')\n",
    "y_pred = sess.run(output, feed_dict).argmax(axis=-1)\n",
    "y_true = mnist.test.labels.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用sklearn 的分類報告, F1分數, 混淆矩陣\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98       992\n",
      "          1       0.99      0.99      0.99      1136\n",
      "          2       0.98      0.98      0.98      1030\n",
      "          3       0.98      0.97      0.98      1018\n",
      "          4       0.98      0.97      0.98       985\n",
      "          5       0.97      0.98      0.98       886\n",
      "          6       0.98      0.98      0.98       957\n",
      "          7       0.97      0.97      0.97      1026\n",
      "          8       0.97      0.98      0.97       972\n",
      "          9       0.96      0.97      0.97       998\n",
      "\n",
      "avg / total       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( classification_report(y_pred, y_true) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們特地使用sklearn的評價報告, 這個報告我們要輸入`估計值`, `實際值`, `類別`.  \n",
    "由於我們的類別就是0~9, 所以就直接省下了類別, 報告內容如下：  \n",
    "1. 精確度 precision: 判斷為該類別中真正為該類別的比例\n",
    "2. 召回率 recall: 真正在該類別中被正確分類的比例\n",
    "3. f1-score: 綜合精確度與召回率的調和值\n",
    "4. support: 該資料集中該類的數量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 970    0    4    0    2    2    4    2    4    4]\n",
      " [   0 1121    0    0    0    1    2    6    0    6]\n",
      " [   1    2 1007    3    2    0    1   10    4    0]\n",
      " [   0    1    3  989    0    8    1    5    6    5]\n",
      " [   1    0    2    0  960    2    6    1    3   10]\n",
      " [   2    2    0    5    0  869    4    0    1    3]\n",
      " [   2    3    2    0    3    4  939    0    3    1]\n",
      " [   1    1    7    4    2    1    0  999    4    7]\n",
      " [   3    5    6    4    0    3    1    0  948    2]\n",
      " [   0    0    1    5   13    2    0    5    1  971]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2092bf643c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAD8CAYAAABErA6HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAF6pJREFUeJzt3XuUXlV5x/HvLwnhlkgQlIYJGqjxQrEqpkilpWi8AFqiXdJGrUQana5VFLSuKuofLKptpSqI1VJHAgVFbhEXEVlcBOKllchdg0GJEWFIILhALoLCzPv0j7NHXtKZeS/znjPn3fP7ZJ015z23Z59M8syeffY+WxGBmZnVx6zpLoCZmT2TE7OZWc04MZuZ1YwTs5lZzTgxm5nVjBOzmVnNODGbmdWME7OZWc04MZuZ1cycsgM8ceUXKhlaOP8v/62KMGZWopEn79VUr/HUrza3nXN22HO/Kccrg2vMZmY1U3qN2cysUo3R6S7BlDkxm1leRkemuwRT5sRsZlmJaEx3EabMidnM8tJwYjYzqxfXmM3MasYP/8zMamYm1JglvRhYDgwAAWwB1kbExpLLZmbWscigV8akA0wkfQS4ABDwQ+CGtH6+pBPLL56ZWYcajfaXmmpVY14F/FFEPNW8UdKpwO3Ap8Y7SdIgMAjwH8evYNWRh/SgqGZmbZgBTRkNYG/gl9ttX5j2jSsihoAhqO5dGWZmwIx4+PcB4BpJdwL3pG3PA14AvK/MgpmZdSX3GnNEXCHphcBBFA//BAwDN0RE//9YMrP8ZPDwr2WvjCjGN15fQVnMzKauxg/12uV+zGaWlRx+mXdiNrO85N7GbGbWd9yUYWZWMxnUmD21lJnlZfSp9pcWJJ0laZukDU3bni3pakl3pq+7p+2S9HlJmyT9SNKBTeesTMffKWllq7hOzGaWl94Oyf5v4PDttp0IXBMRS4Br0meAI4AlaRkEzoAikQMnAa+i6Hp80lgyn0jpTRlVzV79xJbvVRIHYOe9/7yyWNY/qpxuucrhtLWcRnoyPWzKiIjvSlq83eblwGFp/RxgHfCRtP3ciAjgekkLJC1Mx14dEQ8CSLqaItmfP1FctzGbWV7Kf/i3V0RsBYiIrZKem7YP8PQIaSgG4w1Msn1CTsxmlpcOEnPzC9eSofSun26M98tFTLJ9Qk7MZpaVaOOh3u+PbXrhWgful7Qw1ZYXAtvS9mFgn6bjFlG8v36Yp5s+xravmyyAH/6ZWV6i0f7SnbXAWM+KlcClTduPSb0zDgYeTk0eVwJvkLR7euj3hrRtQq4xm1leetjGLOl8itrunpKGKXpXfAq4SNIq4G7g6HT45cCRwCbgceBYgIh4UNInKCYaAfjnsQeBE3FiNrO89LZXxtsn2LVsnGMDOG6C65wFnNVuXCdmM8uLh2SbmdVMBkOynZjNLC8j/f+i/K57ZUg6tpcFMTPrifJ7ZZRuKt3lTp5oh6RBSTdKurHR+M0UQpiZdai378qYFpM2ZUj60US7gL0mOq+50/acuQOeJdvMqlPjmnC7WrUx7wW8EXhou+0C/reUEpmZTUWNa8LtapWYLwPmRcSt2++QtK6UEpmZTUXuNeaIWDXJvnf0vjhmZlOUQa8Md5czs7xE/z/WcmI2s7zMgDZmM7P+4sRsZlYzuT/8MzPrO6Oj012CKcsmMVc5Qepj1/17ZbHmvebDlcWyqZGqm7Y0KnzA1XeP0tyUYWZWM07MZmY14zZmM7N6iUbfNb78P07MZpYXN2WYmdWMe2WYmdWMa8xmZjXjxGxmVjMZvMSo5dRSkl4saZmkedttP7y8YpmZdSmDqaUmTcySjgcuBd4PbJC0vGn3v5ZZMDOzrjSi/aWmWjVlvBd4ZUQ8JmkxsEbS4og4nWJ6KTOzepkBvTJmR8RjABFxl6TDKJLz85kkMUsaBAYBNHs3Zs3atUfFNTObXNS4iaJdrdqY75P08rEPKUm/GdgTeOlEJ0XEUEQsjYilTspmVqkZ0JRxDPCMCbQiYgQ4RtKXSiuVmVm3cn9XRkQMT7Lvf3pfHDOzKapxTbhdLbvLmZn1lZHR9pcWJH1Q0u2SNkg6X9JOkvaVtF7SnZIulDQ3Hbtj+rwp7V/c7S04MZtZXqLR/jIJSQPA8cDSiDgAmA2sAE4BTouIJcBDwKp0yirgoYh4AXBaOq4rTsxmlpfePvybA+wsaQ6wC7AVeC2wJu0/B3hLWl+ePpP2L1OX09o4MZtZVqLRaHuZ9DoR9wKfAe6mSMgPAzcBv06dIACGgYG0PgDck84dScfv0c09ODGbWV46qDFLGpR0Y9MyOHYZSbtT1IL3BfYGdgWOGCfiWNV7vNpxV08i/RIjM8tLB70yImIIGJpg9+uAX0TEAwCSLgFeDSyQNCfVihcBW9Lxw8A+wHBq+tgNeLCbW3Bi7kKVM1c/euXJlcWa/8aTKotV1Xj+KjtOVTlztd+HMIneDcm+GzhY0i7AE8Ay4EbgOuBtwAXASor3CQGsTZ9/kPZfG13+o3BiNrOs9GrOv4hYL2kNcDPFQLtbKGrX3wIukPTJtG11OmU18BVJmyhqyiu6je3EbGZ56eEAk4g4Cdj+V8nNwEHjHPtb4OhexHViNrO8ZPASIydmM8tLBkOynZjNLC9OzGZm9RKjbsowM6sX15jNzOqlV93lplPLxCzpICAi4gZJ+wOHA3dExOWll87MrFO5J2ZJJ1GMDZ8j6WrgVcA64ERJr4iIfym/iGZmHej/JuaWNea3AS8HdgTuAxZFxCOSPg2sB8ZNzJ6M1cymS4z0f2ZulZhHImIUeFzSzyPiEYCIeELShHff/GKQOXMH+v/3CjPrH/2fl1sm5icl7RIRjwOvHNsoaTeyuH0zy81MePh3aET8DiDiGfOw7EDxFiUzs3rJoMrYapbs302w/VfAr0opkZnZFMyEGrOZWX/JvcZsZtZvfj8bXx9zYjazrIRrzGZmNePEbGZWL64xm5nVjBOzla7Kmasf/cY/VRZr/ls/XUmcKmeT7v9OWuObpf6akztG+6u843FiNrOsuMZsZlYz0XCN2cysVlxjNjOrmQjXmM3MasU1ZjOzmmm4V4aZWb344Z+ZWc3kkJhndXqCpHPLKIiZWS9EtL/UVatZstduvwl4jaQFABFxVFkFMzPrRg415lZNGYuAnwBnUow4FbAU+OxkJ3mWbDObLr3sLpcqoWcCB1DkwL8DfgpcCCwG7gL+OiIekiTgdOBI4HHg3RFxczdxWzVlLAVuAj4OPBwR64AnIuI7EfGdiU6KiKGIWBoRS52UzaxKo6Nqe2nD6cAVEfFi4GXARuBE4JqIWAJckz4DHAEsScsgcEa399Bqzr8GcJqki9PX+1udY2Y2nXpVY5b0LOBQ4N3FdeNJ4ElJy4HD0mHnAOuAjwDLgXMjIoDrJS2QtDAitnYau60kGxHDwNGS3gQ80mkQM7OqdNLG3NzsmgxFxFBa3w94ADhb0ssoWg9OAPYaS7YRsVXSc9PxA8A9TdcaTtvKScxjIuJbwLc6DWJmVpVOelukJDw0we45wIHA+yNivaTTebrZYjzj/UToqu9Hx93lzMzqLBpqe2lhGBiOiPXp8xqKRH2/pIUA6eu2puP3aTp/EbClm3twYjazrIw2ZrW9TCYi7gPukfSitGkZRS+1tcDKtG0lcGlaXwsco8LBFB0mOm7GAD/IM7PM9HjgyPuB8yTNBTYDx1JUaC+StAq4Gzg6HXs5RVe5TRTd5Y7tNqgTs5llpdHDfswRcStFt+HtLRvn2ACO60VcJ2Yzy4rfx2xmVjN1fgdGu5yYa67Kn/1VzVwN8Mhnl1cS51kfurT1QTapRp9lul42ZUwXJ2Yzy0qr3hb9wInZzLLSX/X78Tkxm1lW3JRhZlYz7pVhZlYzGUyS7cRsZnmJSvsylcOJ2cyyMuKmDDOzeplxNWZJfwYcBGyIiKvKKZKZWfdyaGOetCe2pB82rb8X+AIwHzhJ0mQvjDYzmxaB2l7qqtUQmR2a1geB10fEycAbgHdOdJKkQUk3Srqx0fhND4ppZtaeRgdLXbVqypglaXeKBK6IeAAgIn4jaWSik5qna5kzdyCHgThm1idGa1wTblerxLwbxQSEAkLSH0TEfZLmUe37dczM2tLBXKy1NWlijojFE+xqAG/teWnMzKaokUGdsavuchHxOPCLHpfFzGzKcmg7dT9mM8tKnR/qtcuJ2cyy0tAMbcowM6ur0ekuQA84MZtZVrLvlWFm1m9mbK8Mq04OT5jHU9UkqY9ecFwlcQDmr/hiZbFsYjn8n3FiNrOsuCnDzKxm3F3OzKxmRl1jNjOrF9eYzcxqxonZzKxmMpjyr+WL8s3M+kqvX5QvabakWyRdlj7vK2m9pDslXShpbtq+Y/q8Ke1f3O09ODGbWVZGO1jadAKwsenzKcBpEbEEeAhYlbavAh6KiBcAp6XjutJqzr9XSXpWWt9Z0smSvinpFEm7dRvUzKwsDbW/tCJpEfAm4Mz0WcBrgTXpkHOAt6T15ekzaf+ydHzHWtWYzwIeT+unU8xockradnY3Ac3MytRJU0bz/KRpGdzucp8DPszTLR97AL+OiLGp9YaBgbQ+ANwDkPY/nI7vWMs5/5oKsDQiDkzr35d0azcBzczK1EmvjOb5Sbcn6c3Atoi4SdJhY5vHu0wb+zrSqsa8QdKxaf02SUsBJL0QeGqikzxLtplNl+hgaeEQ4ChJdwEXUDRhfA5YIGmsUrsI2JLWh4F9ANL+3YAHu7mHVon5PcBfSPo5sD/wA0mbgS+nfeOKiKGIWBoRS2fN2rWbcpmZdaVXbcwR8dGIWJTmPl0BXBsR7wSuA96WDlsJjL2Ra236TNp/bUR0VWNuNRnrw8C7Jc0H9kvHD0fE/d0EMzMrWwUvyv8IcIGkTwK3AKvT9tXAVyRtoqgpr+g2QFsDTCLiUeC2boOYmVWlUcKLPyNiHbAurW8GDhrnmN8CR/cinkf+mVlWPCTbzKxm/KJ8M7OacY3ZzKxmRtT/dWYnZjPLSv+nZSdmM8uMmzJmqCpf95rDT//pVOXM1Y9d86nKYs1bdmJlsfrt9cZldJermhOzmWWl/9OyE7OZZcZNGWZmNTOaQZ3ZidnMsuIas5lZzYRrzGZm9eIas5lZzbi7nJlZzfR/Wm49S/bxkvapqjBmZlM1QrS91FWrqaU+AayX9D1J/yDpOVUUysysW9HBn7pqlZg3U0w2+AnglcBPJF0haWWabmpcnozVzKZLo4Olrlol5oiIRkRcFRGrgL2B/wQOp0jaE53kyVjNbFrkUGNu9fDvGe8viYinKGaCXStp59JKZWbWpTrXhNvVKjH/zUQ7IuKJHpfFzGzKRqO+NeF2TZqYI+JnVRXEzKwX3I/ZzKxm6tx23C4nZjPLykxoYzYz6ytuyjAzqxk3ZZiZ1Uz2vTLMzPqNmzJqpN9m8rX8VDlz9aMXn1BZrPlHn15ZrF7I4eFfqyHZZmZ9pVdDsiXtI+k6SRsl3S7phLT92ZKulnRn+rp72i5Jn5e0SdKPJB3Y7T04MZtZVhpE20sLI8CHIuIlwMHAcZL2B04EromIJcA16TPAEcCStAwCZ3R7D07MZpaViGh7aXGdrRFxc1p/FNgIDADLgXPSYecAb0nry4Fzo3A9sEDSwm7uIZs2ZjMzgNESHv5JWgy8AlgP7BURW6FI3pKemw4bAO5pOm04bdvaaTzXmM0sK500ZTS/Oz4tg9tfT9I84OvAByLikUlCj9cHoaufEq4xm1lWWjVRbHfsEDA00X5JO1Ak5fMi4pK0+X5JC1NteSGwLW0fBpqn4lsEbOmk7GNcYzazrPTq4Z8kAauBjRFxatOutcDKtL4SuLRp+zGpd8bBwMNjTR6dco3ZzLLSwyHZhwDvAn4s6da07WPAp4CLJK0C7gaOTvsuB44ENgGPA8d2G3jSxCxpLrAC2BIR35b0DuDVFE8nh9KMJmZmtdGrIdkR8X0mHru2bJzjAziuF7Fb1ZjPTsfsImklMA+4JBXqIJ6uzpuZ1cJMGJL90oj4Y0lzgHuBvSNiVNJXgdsmOik92RwE0Ozd8ISsZlaVHBJzq4d/s1JzxnxgF2C3tH1HYIeJTvIs2WY2XXo1wGQ6taoxrwbuAGYDHwculrSZYnjiBSWXzcysYznUmFtNxnqapAvT+hZJ5wKvA74cET+sooBmZp2YES/Kj4gtTeu/BtaUWiIzsykYjf5/8af7MZtZVurcdtwuJ2Yzy0r2bcxmZv1mRrQxm5n1k4abMszM6sU1ZjOzmnGvjBrp/5+R46ty9u/iLYfVyOHXze1V+b2qcubqR7/50cpi9UIO/7ayScxmZuCmDDOz2nGN2cysZlxjNjOrmdEYne4iTJkTs5llxUOyzcxqxkOyzcxqxjVmM7OamRG9MiT9IfBWYB9gBLgTOD8iHi65bGZmHcuhV8akc/5JOh74L2An4E+AnSkS9A8kHVZ66czMOjQajbaXumpVY34v8PI0M/apwOURcZikLwGXAq8Y7yTPkm1m02WmtDHPAUYpZsaeDxARd0uadJZsYAhgztyB/v9bMrO+MRPamM8EbpB0PXAocAqApOcAD5ZcNjOzjmVfY46I0yV9G3gJcGpE3JG2P0CRqM3MamVG9GOOiNuB2ysoi5nZlGVfYzYz6zd17m3RLidmM8tKDg//Ju3HbGbWbyKi7aUVSYdL+qmkTZJOrKD4gBOzmWUmOvgzGUmzgS8CRwD7A2+XtH8Ft+DEbGZ56WGN+SBgU0RsjogngQuA5aXfAG5jNrPM9LCNeQC4p+nzMPCqXl18MqUn5pEn7+1q8mBJg2kEYamqiuNY/RUrx3vKOVazTnJO8+sjkqGmMo93nUqeLNa5KWOw9SF9Fcex+itWjveUc6yuRMRQRCxtWpp/kAxTvLRtzCJgSxXlqnNiNjObTjcASyTtK2kusAJYW0VgtzGbmY0jIkYkvQ+4EpgNnJVGQpeuzom5qrapKtvAHKt/YuV4TznHKkVEXA5cXnVc5TCu3MwsJ25jNjOrmdol5qqGQEo6S9I2SRvKitEUax9J10naKOl2SSeUGGsnST+UdFuKdXJZsVK82ZJukXRZyXHukvRjSbdKurHkWAskrZF0R/qe/WlJcV6U7mdseUTSB0qK9cH072GDpPMl7VRGnBTrhBTn9rLuJ3udjJIpe6FoYP85sB8wF7gN2L+kWIcCBwIbKrivhcCBaX0+8LMS70vAvLS+A7AeOLjEe/tH4GvAZSX/Hd4F7Fn29yrFOgd4T1qfCyyoIOZs4D7g+SVcewD4BbBz+nwR8O6S7uMAYAOwC8UzrG8DS6r4vuW01K3GXNkQyIj4LhXNwhIRWyPi5rT+KLCR4j9LGbEiIh5LH3dISykPEiQtAt5EMdNNFiQ9i+KH9mqAiHgyIn5dQehlwM8j4pclXX8OsLOkORRJs6z+uC8Bro+IxyNiBPgO8NaSYmWrbol5vCGQpSSw6SJpMcUktutLjDFb0q3ANuDqiCgr1ueADwNVvAA3gKsk3ZRGa5VlP+AB4OzURHOmpCpmE14BnF/GhSPiXuAzwN3AVuDhiLiqjFgUteVDJe0haRfgSJ45SMPaULfEPG1DIKsgaR7wdeADEfFIWXEiYjQiXk4xUukgSQf0OoakNwPbIuKmXl97AodExIEUb/o6TlJZU5vNoWjiOiMiXgH8Bij1dY9p8MJRwMUlXX93it889wX2BnaV9LdlxIqIjRRzg14NXEHRHDlSRqyc1S0xT9sQyLKlWcW/DpwXEZdUETP9Cr4OOLyEyx8CHCXpLoomp9dK+moJcQCIiC3p6zbgGxTNXmUYBoabfstYQ5Goy3QEcHNE3F/S9V8H/CIiHoiIp4BLgFeXFIuIWB0RB0bEoRTNhXeWFStXdUvM0zYEskySRNFmuTEiTi051nMkLUjrO1P8p7yj13Ei4qMRsSgiFlN8n66NiFJqYZJ2lTR/bB14A8WvzD0XEfcB90h6Udq0DPhJGbGavJ2SmjGSu4GDJe2S/i0uo3jOUQpJz01fnwf8FeXeW5ZqNfIvKhwCKel84DBgT0nDwEkRsbqMWBS1y3cBP05tvwAfi2JUUa8tBM5JL/meBVwUEaV2ZavAXsA3ipzCHOBrEXFFifHeD5yXKgebgWPLCpTaYV8P/H1ZMSJivaQ1wM0UzQq3UO6ovK9L2gN4CjguIh4qMVaWPPLPzKxm6taUYWY24zkxm5nVjBOzmVnNODGbmdWME7OZWc04MZuZ1YwTs5lZzTgxm5nVzP8BdJ9ricxkICwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 用估計值和真實值製作混淆矩陣\n",
    "mat = confusion_matrix(y_pred, y_true)\n",
    "print(mat)\n",
    "# 用熱點圖可以清楚看出混淆矩陣的分布\n",
    "sns.heatmap(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們透過混淆矩陣, 可以確認是不是有某一種類別分類特別不好.  \n",
    "不過, 混淆矩陣並不能很好的表達到底模型好或不好, 真的很混淆.  \n",
    "所以通常我們都會使用`分類報告 classification report`.  \n",
    "實務上我們可能會特別重視分類是否正確(或錯誤), 因此可能需要調整`f1-score`計算.  \n",
    "不過那太複雜了, 之後有碰到在寫.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要記得關掉會話哦~\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
